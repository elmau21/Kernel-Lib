# Kernel - Motor Avanzado de MÃ©todos de Kernel para Machine Learning e IngenierÃ­a

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: Proprietary](https://img.shields.io/badge/license-Proprietary-red.svg)](LICENSE)

**Kernel** es una biblioteca de alto rendimiento y producciÃ³n para mÃ©todos de kernel en machine learning e ingenierÃ­a, diseÃ±ada para ser vendida como producto SaaS o librerÃ­a premium.

## ğŸ¯ CaracterÃ­sticas Principales

### âœ… Implementado y Listo para ProducciÃ³n

#### MÃ©todos de Machine Learning Completos

- âœ… **Support Vector Machines (SVM)** con algoritmo SMO optimizado
- âœ… **Kernel Principal Component Analysis (KPCA)** con reducciÃ³n de dimensionalidad
- âœ… **Gaussian Processes** para regresiÃ³n con incertidumbre
- âœ… OptimizaciÃ³n automÃ¡tica de hiperparÃ¡metros

#### Redes Neuronales Implementadas MatemÃ¡ticamente desde Cero

- âœ… **Red Neuronal Multicapa** con forward/backward propagation completo
- âœ… **Backpropagation** implementado matemÃ¡ticamente (regla de la cadena)
- âœ… **Funciones de ActivaciÃ³n**:
  - ClÃ¡sicas: Sigmoid, Tanh, ReLU, LeakyReLU, ELU, Softmax, Linear
  - Avanzadas: Softplus, Swish, HardSigmoid, HardSwish, GELU, PReLU, SELU, Mish
- âœ… **Funciones de PÃ©rdida**: MSE, Cross-Entropy, Binary Cross-Entropy
- âœ… **RegularizaciÃ³n**: L1, L2, Dropout
- âœ… **InicializaciÃ³n**: Xavier, He, Random

#### Optimizadores MatemÃ¡ticos (40+ Implementados)

- âœ… **ClÃ¡sicos**: SGD, Momentum, RMSprop, Adam, AdaGrad, Nesterov
- âœ… **Adaptativos Avanzados**: AdamW, Nadam, RAdam, AdaBelief, AdaMax, Yogi
- âœ… **Especializados**: Lion, Ranger, RangerQH, Lamb, QHM
- âœ… **Variantes**: Adadelta, Rprop, SignSGD, Adafactor, NovoGrad
- âœ… **HÃ­bridos**: Lookahead, AggMo, AdaMod, SMORMS3, AdaShift
- âœ… **Con Restricciones**: AdaBound, AMSBound
- âœ… **Otros**: Fromage, AddSign, PowerSign, ExtendedRprop

#### Algoritmos de OptimizaciÃ³n MatemÃ¡tica Avanzada

- âœ… **Gradiente Descendente** con bÃºsqueda de lÃ­nea (Armijo, Wolfe)
- âœ… **MÃ©todo de Newton** con cÃ¡lculo de Hessiana
- âœ… **Gradiente Conjugado** para sistemas lineales
- âœ… **BFGS** (Quasi-Newton method)

#### Ãlgebra Lineal Computacional

- âœ… **Descomposiciones**: LU, QR, SVD, Cholesky
- âœ… **Solvers de Sistemas Lineales**: LU, QR, Cholesky
- âœ… **Autovalores y Autovectores**: Power Method, QR Algorithm
- âœ… **Pseudoinversa de Moore-Penrose**

#### Kernels Avanzados

- âœ… **RBF (Gaussian)** - Optimizado con estabilidad numÃ©rica
- âœ… **Polynomial** - HomogÃ©neo e inhomogÃ©neo
- âœ… **Linear** - Altamente optimizado con BLAS
- âœ… **Matern** - Con parÃ¡metro de suavidad Î½ (0.5, 1.5, 2.5, âˆ)
- âœ… **Laplacian** - Robusto a outliers
- âœ… **Composite** - Suma, producto y combinaciones lineales
- âœ… **Scaled** - Transformaciones de escala
- âœ… **Custom** - Kernels personalizados

#### Optimizaciones Avanzadas

- âœ… **Sistema de Caching LRU** - CachÃ© inteligente con hash de datos
- âœ… **DescomposiciÃ³n de Cholesky** - Para sistemas lineales eficientes
- âœ… **Eigendecomposition** - Para anÃ¡lisis espectral
- âœ… **Soporte GPU** - CuPy para aceleraciÃ³n CUDA/OpenCL
- âœ… **Estabilidad NumÃ©rica** - Manejo robusto de casos edge
- âœ… **ValidaciÃ³n MatemÃ¡tica** - VerificaciÃ³n de propiedades PSD

#### API REST Completa

- âœ… **FastAPI** - API REST moderna y rÃ¡pida
- âœ… **Endpoints para Kernels** - CÃ¡lculo de matrices de kernel
- âœ… **Endpoints para SVM** - Entrenamiento y predicciÃ³n
- âœ… **Endpoints para KPCA** - TransformaciÃ³n de datos
- âœ… **Endpoints para GP** - RegresiÃ³n con incertidumbre
- âœ… **GestiÃ³n de Modelos** - Almacenamiento y recuperaciÃ³n

## ğŸš€ Inicio RÃ¡pido

### InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/kernel-ml/kernel.git
cd kernel

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### Uso BÃ¡sico

```python
from kernel.kernels.rbf import RBFKernel
from kernel.methods.svm import KernelSVM
import numpy as np

# Crea kernel RBF
kernel = RBFKernel(gamma=1.0, use_cache=True)

# Calcula matriz de kernel
X = np.random.randn(100, 10)
K = kernel.gram_matrix(X)

# Entrena SVM
svm = KernelSVM(kernel=kernel, C=1.0)
X_train, y_train = np.random.randn(50, 2), np.random.choice([-1, 1], 50)
svm.fit(X_train, y_train)

# Predice
X_test = np.random.randn(20, 2)
predictions = svm.predict(X_test)
```

### API REST

```bash
# Iniciar servidor
cd api
python main.py

# O con uvicorn
uvicorn api.main:app --reload --port 8000
```

**DocumentaciÃ³n interactiva**: http://localhost:8000/docs

**Ejemplo de uso de API**:

```bash
# Calcular kernel
curl -X POST "http://localhost:8000/kernels/compute" \
  -H "Content-Type: application/json" \
  -d '{
    "X": [[1.0, 2.0], [3.0, 4.0]],
    "kernel_config": {
      "type": "rbf",
      "params": {"gamma": 1.0}
    }
  }'
```

## ğŸ“š Ejemplos Avanzados

### Ejemplo 1: SVM con Kernel RBF

```python
from kernel.kernels.rbf import RBFKernel
from kernel.methods.svm import KernelSVM
from sklearn.datasets import make_circles

# Datos no linealmente separables
X, y = make_circles(n_samples=200, noise=0.1, factor=0.5)
y = np.where(y == 0, -1, 1)

# Entrena SVM
kernel = RBFKernel(gamma=1.0)
svm = KernelSVM(kernel=kernel, C=1.0, tol=1e-3)
svm.fit(X, y)

# Predice
predictions = svm.predict(X)
accuracy = svm.score(X, y)
print(f"Accuracy: {accuracy:.4f}")
```

### Ejemplo 2: ReducciÃ³n de Dimensionalidad con KPCA

```python
from kernel.methods.kpca import KernelPCA
from kernel.kernels.rbf import RBFKernel

# Datos de alta dimensionalidad
X = np.random.randn(100, 50)

# Aplica KPCA
kernel = RBFKernel(gamma=0.1)
kpca = KernelPCA(kernel=kernel, n_components=2, center_kernel=True)
X_reduced = kpca.fit_transform(X)

# Varianza explicada
explained_var = kpca.explained_variance_ratio_()
print(f"Varianza explicada: {explained_var}")
```

### Ejemplo 3: RegresiÃ³n con Gaussian Process

```python
from kernel.methods.gaussian_process import GaussianProcess
from kernel.kernels.rbf import RBFKernel

# Datos de entrenamiento
X_train = np.linspace(0, 10, 50).reshape(-1, 1)
y_train = np.sin(X_train.ravel()) + np.random.randn(50) * 0.1

# Entrena GP
kernel = RBFKernel(gamma=1.0)
gp = GaussianProcess(kernel=kernel, alpha=0.1, normalize_y=True)
gp.fit(X_train, y_train)

# Predice con incertidumbre
X_test = np.linspace(0, 10, 100).reshape(-1, 1)
y_mean, y_std = gp.predict(X_test, return_std=True)
```

### Ejemplo 4: Kernels Compuestos

```python
from kernel.kernels.rbf import RBFKernel
from kernel.kernels.polynomial import PolynomialKernel
from kernel.kernels.composite import CompositeKernel, ScaledKernel

# Kernel compuesto: RBF + Polynomial
rbf = RBFKernel(gamma=1.0)
poly = PolynomialKernel(degree=2, gamma=0.1)
composite = CompositeKernel([rbf, poly], operation="sum")

# Kernel escalado
scaled = ScaledKernel(rbf, scale=2.0)

# Usa en modelos
X = np.random.randn(100, 10)
K = composite.gram_matrix(X)
```

### Ejemplo 5: Red Neuronal desde Cero

```python
from kernel.math.neural_network import NeuralNetwork
from kernel.math.activations import ReLU, Sigmoid
from kernel.math.activations import BinaryCrossEntropy

# Crea red neuronal: 20 -> 64 -> 32 -> 1
nn = NeuralNetwork(
    layers=[20, 64, 32, 1],
    activations=[ReLU(), ReLU(), Sigmoid()],
    loss=BinaryCrossEntropy(),
    weight_init="he"
)

# Entrena
nn.fit(X_train, y_train, epochs=100, batch_size=32, learning_rate=0.01)

# Predice
predictions = nn.predict(X_test)
accuracy = nn.evaluate(X_test, y_test)["accuracy"]
```

### Ejemplo 6: OptimizaciÃ³n MatemÃ¡tica

```python
from kernel.math.optimization import GradientDescent, NewtonMethod, BFGS

# Define funciÃ³n objetivo y gradiente
def f(x):
    return (x[0] - 1)**2 + (x[1] - 2)**2

def grad_f(x):
    return np.array([2*(x[0] - 1), 2*(x[1] - 2)])

# Optimiza con gradiente descendente
optimizer = GradientDescent(max_iter=1000, line_search="armijo")
result = optimizer.minimize(f, grad_f, x0=np.array([0.0, 0.0]))
print(f"Ã“ptimo: {result['x']}, Valor: {result['fun']}")
```

### Ejemplo 7: Ãlgebra Lineal Computacional

```python
from kernel.math.linear_algebra import (
    MatrixDecomposition, EigenvalueSolver, LinearSystemSolver
)

# DescomposiciÃ³n LU
A = np.random.randn(5, 5)
L, U, P = MatrixDecomposition.lu_decomposition(A)

# Resuelve sistema lineal
b = np.random.randn(5)
x = LinearSystemSolver.solve_lu(A, b)

# Autovalores con Power Method
eigenvalue, eigenvector = EigenvalueSolver.power_method(A)
```

## ğŸ“ Fundamentos MatemÃ¡ticos

### TeorÃ­a de Kernels

Un **kernel** K(x, y) es una funciÃ³n que mide la similitud entre dos vectores en un espacio de caracterÃ­sticas de alta dimensiÃ³n. Un kernel vÃ¡lido debe ser:

1. **SimÃ©trico**: K(x, y) = K(y, x)
2. **Positivo Semidefinido (PSD)**: Para cualquier conjunto de puntos, la matriz de Gram es PSD

### Reproducing Kernel Hilbert Spaces (RKHS)

Cada kernel vÃ¡lido define un espacio de Hilbert de funciones donde el kernel actÃºa como producto interno:

```
<f, K(Â·, x)> = f(x)  (Propiedad de reproducciÃ³n)
```

### Teorema de Mercer

Si K es un kernel vÃ¡lido, existe un mapeo Ï†: X â†’ H tal que:

```
K(x, y) = <Ï†(x), Ï†(y)>_H
```

donde H es un espacio de Hilbert.

## ğŸ› ï¸ Arquitectura TÃ©cnica

### Optimizaciones Implementadas

1. **Caching LRU**: Sistema de cachÃ© con hash SHA256 de datos y parÃ¡metros
2. **Cholesky Decomposition**: Para resolver sistemas lineales O(nÂ³) â†’ O(nÂ²)
3. **Eigendecomposition**: Para anÃ¡lisis espectral y KPCA
4. **GPU Acceleration**: Soporte opcional con CuPy
5. **Numerical Stability**: Manejo de casos edge y underflow

### Algoritmos

- **SMO (Sequential Minimal Optimization)**: Para SVM, mÃ¡s eficiente que QP general
- **Kernel Centering**: Para KPCA, centra la matriz de kernel
- **Marginal Likelihood**: Para optimizaciÃ³n de hiperparÃ¡metros en GP

## ğŸ“Š Benchmarks

Ejecuta benchmarks de rendimiento:

```bash
python benchmarks/performance_test.py
```

EvalÃºa:

- Velocidad de cÃ¡lculo de kernels
- Escalabilidad con tamaÃ±o de datos
- ComparaciÃ³n CPU vs GPU
- Uso de memoria

## ğŸ§ª Testing

```bash
# Ejecutar todos los tests
pytest tests/

# Con cobertura
pytest tests/ --cov=kernel --cov-report=html

# Tests especÃ­ficos
pytest tests/test_kernels.py -v
```

## ğŸ“– DocumentaciÃ³n

### Estructura del Proyecto

```
Kernel/
â”œâ”€â”€ kernel/                  # CÃ³digo principal
â”‚   â”œâ”€â”€ core/               # NÃºcleo matemÃ¡tico avanzado
â”‚   â”‚   â””â”€â”€ kernel_base.py  # Clase base con optimizaciones
â”‚   â”œâ”€â”€ kernels/            # ImplementaciÃ³n de kernels
â”‚   â”‚   â”œâ”€â”€ rbf.py          # RBF optimizado
â”‚   â”‚   â”œâ”€â”€ polynomial.py   # Polinomial
â”‚   â”‚   â”œâ”€â”€ linear.py       # Lineal
â”‚   â”‚   â”œâ”€â”€ matern.py        # Matern
â”‚   â”‚   â”œâ”€â”€ laplacian.py    # Laplaciano
â”‚   â”‚   â””â”€â”€ composite.py    # Kernels compuestos
â”‚   â”œâ”€â”€ methods/            # MÃ©todos de ML
â”‚   â”‚   â”œâ”€â”€ svm.py          # SVM con SMO
â”‚   â”‚   â”œâ”€â”€ kpca.py         # Kernel PCA
â”‚   â”‚   â””â”€â”€ gaussian_process.py  # Gaussian Process
â”‚   â””â”€â”€ math/               # Algoritmos matemÃ¡ticos avanzados
â”‚       â”œâ”€â”€ activations.py  # Funciones de activaciÃ³n
â”‚       â”œâ”€â”€ neural_network.py  # Redes neuronales desde cero
â”‚       â”œâ”€â”€ optimizers.py   # Optimizadores (SGD, Adam, etc.)
â”‚       â”œâ”€â”€ optimization.py # OptimizaciÃ³n matemÃ¡tica
â”‚       â””â”€â”€ linear_algebra.py  # Ãlgebra lineal computacional
â”œâ”€â”€ api/                    # API REST
â”‚   â””â”€â”€ main.py             # FastAPI application
â”œâ”€â”€ tests/                  # Tests unitarios
â”œâ”€â”€ examples/               # Ejemplos de uso
â”‚   â”œâ”€â”€ advanced_usage.py   # Ejemplos avanzados
â”‚   â””â”€â”€ neural_network_example.py  # Ejemplos de redes neuronales
â””â”€â”€ benchmarks/            # Benchmarks
    â””â”€â”€ performance_test.py
```
